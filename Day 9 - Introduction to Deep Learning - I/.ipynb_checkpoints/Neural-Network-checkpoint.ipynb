{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RiWOqVGJpnR5",
    "outputId": "fdde8e36-e354-4943-d027-e8a691e184ec"
   },
   "source": [
    "# Introduction to Deep Learning - I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kJQgCx-EpnRm"
   },
   "source": [
    "Created by [Nimblebox Inc.](https://www.nimblebox.ai/).\n",
    "\n",
    "<img style=\"float:left; margin-left: 50px\" src=\"https://databricks.com/wp-content/uploads/2019/02/neural1.jpg\" alt=\"Numpy Logo\" width=\"300\" height=\"400\">\n",
    "\n",
    "<img style=\"float:right; margin-right: 50px\" src=\"https://media-exp1.licdn.com/dms/image/C4E1BAQH3ErUUfLXoHQ/company-background_10000/0?e=2159024400&v=beta&t=9Z2hcX4LqsxlDd2BAAW8xDc-Obfvk_rziT1AkPKBcCc\" alt=\"Nimblebox Logo\" width=\"500\" height=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pobAbXhxpnRm"
   },
   "source": [
    "## Introduction:  \n",
    "\n",
    "Till now we looked at Traditional Statistical Machine Learning models like Support Vector Machines, different Linear and Logistic Models, Random Forests, etc.\n",
    "\n",
    "As we have seen, Machine Learning is set of algorithms that parse data, learn from them, and then apply what they’ve learned to make intelligent decisions. Whereas, Deep Learning achieves great power and flexibility by learning to represent the world as nested hierarchy of concepts, with each concept defined in relation to simpler concepts, and more abstract representations computed in terms of less abstract ones.\n",
    "\n",
    "<div style=\"text-align: center\"><img src=\"https://i2.wp.com/semiengineering.com/wp-content/uploads/2018/01/MLvsDL.png\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Deep Learning?\n",
    "\n",
    "Deep learning–a machine learning technique–is an efficient way of learning that relies on big data, where features that can help a machine map an input to an output is automatically extracted from layers of “neurons”. A neural network is a type of deep learning architecture and in this webinar, we will be focusing on a simple Aritficial Neural Network.\n",
    "\n",
    "### What is Neural Network?\n",
    "\n",
    "Neural networks are composed of simple building blocks called neurons. A neuron is a mathematical function that takes data as input, performs a transformation on them, and produces an output. This means that neurons can represent any mathematical function; however, in neural networks, we typically use non-linear functions.\n",
    "\n",
    "<figure style=\"text-align: center\"><img src=\"https://miro.medium.com/max/702/1*NZc0TcMCzpgVZXvUdEkqvA.png\" width=\"400\" height=\"500\"><figcaption>A single neuron in a network</figcaption></figure>\n",
    "\n",
    "Looking at the neuron above, you can see that it’s composed of two main parts: the summation and the activation function. A neuron takes data (x₁, x₂, x₃) as input, multiplies each with a specific weight (w₁, w₂, w₃), and then passes the result to a nonlinear function called the activation function to produce an output.\n",
    "\n",
    "A neural network combines multiple neurons by stacking them vertically/horizontally to create a network of neurons-hence the name “neural network”. A simple one-neuron network is called a perceptron and is the simplest network ever.\n",
    "\n",
    "### Layers of Neural Network\n",
    "\n",
    "1. The first layer is called the input layer, and the number of nodes will depend on the number of features present in your dataset.\n",
    "2. The final layer of the neural network is called the output layer, and the number depends on what you’re trying to predict. For regression and binary classification tasks, you can use a single node; while for multi-class problems, you’ll use multiple nodes, depending on the number of classes.\n",
    "3. The layers between the input and the final layer is where the magic happens— these are called the hidden layers. The hidden layers can be as deep or wide as you want, and while a deeper network is better, the computational time also increases as you go deeper.\n",
    "\n",
    "<div style=\"text-align: center\"><img src=\"https://miro.medium.com/max/702/1*Z3zHoX1nhK6Rsmd4yNPdsg.jpeg\" width=\"500\" height=\"600\"></div>\n",
    "\n",
    "### Weights and Bias\n",
    "\n",
    "Weights and biases are the learnable parameters that help a neural network correctly learn a function. Think of weights as a measure of how sure you are that a feature contributes to a prediction and the bias as a base value that your predictions must start from.\n",
    "\n",
    "A machine learning model uses lots of examples to learn the correct weights and bias to assign to each feature in a dataset to help it correctly predict outputs.\n",
    "\n",
    "### Activation Function\n",
    "\n",
    "Activations are the nonlinear computations done in each node of a Neural Network. There are many types of activation functions used in deep learning - some popular ones are Sigmoid, ReLU, tanh, Leaky ReLU, and so on.\n",
    "\n",
    "<div style=\"text-align: center\"><img src=\"https://miro.medium.com/max/702/1*ZafDv3VUm60Eh10OeJu1vw.png\" width=\"600\" height=\"700\"></div>\n",
    "\n",
    "The hidden layer receives values from the previous layer, calculates a weighted sum, adds the bias term, and then passes each result through an activation function and continues till it reaches the last but one hidden layer. The result from this last but one layer is then passed to the output layer, where another weighted sum is performed using the second weights and biases. But then instead of passing the result through another activation function, it is passed through an output activation function.\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "The loss function is a way of measuring how good a model’s prediction is so that it can adjust the weights and biases. A loss function must be properly designed so that it can correctly penalize a model that is wrong and reward a model that is right. This means that you want the loss to tell you if a prediction made is far or close to the true prediction. The choice of the loss function is dependent on the task—and for classification problems, you can use cross-entropy loss.\n",
    "\n",
    "$$CE = -\\sum_{i}^{C} y_i log(\\hat y_i)$$\n",
    "\n",
    "here $C$ is the number of classes, $y_i$ is the true value and $\\hat y_i$ is the predicted value\n",
    "\n",
    "### Forward Propagation\n",
    "\n",
    "Forward propagation is the name given to the series of computations performed by the neural network before a prediction is made. In a two-layer network, it will perform the following computation for forward propagation:\n",
    "\n",
    "1. Compute the weighted sum between the input and the first layer's weights and then add the bias: **Z1 = (W1 * X) + b**\n",
    "2. Pass the result through the ReLU activation function: **A1 = sigmoid(Z1)**\n",
    "3. Compute the weighted sum between the output (A1) of the previous step and the second layer's weights—also add the bias: **Z2 = (W2 * A1) + b2**\n",
    "4. Compute the output function by passing the result through a sigmoid function: **A2 = sigmoid(Z2)**\n",
    "5. And finally, compute the loss between the predicted output and the true labels: **loss(A2, Y)**\n",
    "\n",
    "Note: For a three-layer neural network, you’d have to compute Z3 and A2 using W3 and b3 before the output layer.\n",
    "\n",
    "### Backpropogation\n",
    "\n",
    "Backpropagation is the name given to the process of training a neural network by updating its weights and bias.\n",
    "\n",
    "A neural network learns to predict the correct values by continuously trying different values for the weights and then comparing the losses. If the loss function decreases, then the current weight is better than the previous, or vice versa. This means that the neural net has to go through many training (forward propagation) and update (backpropagation) cycles in order to get the best weights and biases. This cycle is what we generally refer to as the training phase, and the process of searching for the right weights is called optimization.\n",
    "\n",
    "The way we do this by using chain rule to find the local gradient with respect to each input.\n",
    "\n",
    "This is very big and vast topic, thus I will recommend you to go throught this [original paper on the topic](https://page.mi.fu-berlin.de/rojas/neural/chapter/K7.pdf) or watch different tutorials to clear this concept.\n",
    "\n",
    "### Training Neural Networks\n",
    "\n",
    "To automatically use this information to update the weights and biases, a neural network must perform hundreds, thousands, and even millions of forward and backward propagations. That is, in the training phase, the neural network must perform the following:\n",
    "1. Forward propagation\n",
    "2. Backpropagation\n",
    "3. Weight updates with calculated gradients\n",
    "4. Repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "class NeuralNetwork(object):\n",
    "    def __init__(self, *sizes):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.b = [np.random.uniform(-0.12, 0.12, (1, n)) for n in sizes[1:]]\n",
    "        self.w = [np.random.uniform(-0.12, 0.12, (n, m)) for n, m in zip(sizes[:-1], sizes[1:])]\n",
    "        # self.b = [ np.random.randn(1, n) for n in sizes[1:] ]\n",
    "        # self.w = [ np.random.randn(n, m) for n, m in zip(sizes[:-1], sizes[1:]) ]\n",
    "        self.a = [[] for i in range(self.num_layers)]\n",
    "        self.z = [[] for i in range(self.num_layers-1)]\n",
    "\n",
    "        self.training_size = 0\n",
    "        self.reg_lambda = 1.0\n",
    "\n",
    "        self.grad_w = [[] for i in range(self.num_layers-1)]\n",
    "        self.grad_b = [[] for i in range(self.num_layers-1)]\n",
    "\n",
    "    def feedForward(self, X):\n",
    "        self.a[0] = X\n",
    "        for i in range(self.num_layers-1):\n",
    "            self.z[i] = self.a[i].dot(self.w[i]) + self.b[i]\n",
    "            self.a[i+1] = self.sigmoid(self.z[i])\n",
    "\n",
    "    def cost(self, y):\n",
    "        diff_Ks = -y * np.log(self.a[-1]) - (1 - y) * np.log(1 - self.a[-1])\n",
    "        J = sum(np.sum(diff_Ks, axis=0)) / self.training_size\n",
    "\n",
    "        # regularization\n",
    "        totalSum = np.array([])\n",
    "        for w in self.w:\n",
    "            totalSum = np.concatenate((totalSum, sum(w ** 2.0)))\n",
    "        totalSum = sum(totalSum)\n",
    "        J = J + ((self.reg_lambda * totalSum) / (2.0 * self.training_size))\n",
    "\n",
    "        return J\n",
    "\n",
    "    def backpropagation(self, y):\n",
    "        batch_size = float(len(y))\n",
    "        delta = (self.a[-1] - y)\n",
    "        self.grad_w[-1] = ((delta.T).dot(self.a[-2])) / batch_size\n",
    "        self.grad_b[-1] = np.sum(delta, axis=0) / batch_size\n",
    "        for i in range(2, self.num_layers):\n",
    "            delta = delta.dot(self.w[-i+1].T) * self.sigmoidPrime(self.z[-i])\n",
    "            self.grad_w[-i] = ((delta.T).dot(self.a[-i - 1])) / batch_size\n",
    "            self.grad_b[-i] = np.sum(delta, axis=0) / batch_size\n",
    "\n",
    "        # regularization\n",
    "        for i in range(self.num_layers-1):\n",
    "            self.grad_w[i] = self.grad_w[i].T + (self.reg_lambda * self.w[i]) / self.training_size\n",
    "\n",
    "    def gradientDescent(self, X, y, regularization, learning_rate, epochs, output=False):\n",
    "        self.training_size = float(len(X))\n",
    "\n",
    "        for e in range(epochs):\n",
    "            self.feedForward(X)\n",
    "            self.backpropagation(y)\n",
    "\n",
    "            # update weights\n",
    "            for l in range(self.num_layers-1):\n",
    "                self.w[l] = self.w[l] - learning_rate * self.grad_w[l]\n",
    "                self.b[l] = self.b[l] - learning_rate * self.grad_b[l]\n",
    "            \n",
    "            if output:\n",
    "                predictions = np.argmax(self.a[-1], axis=1)\n",
    "                precision = self.evaluatePredictions(predictions, np.nonzero(y)[1])\n",
    "                print(\"Epoch: {0} - precision: {1:.4f}, cost: {2:.4f}\".format(e, precision, self.cost(y)))\n",
    "\n",
    "\n",
    "    def stochasticGradientDescent(self, X, y, regularization, learning_rate, epochs, batch_size, output=False):\n",
    "        self.training_size = float(len(X))\n",
    "        for e in range(epochs):\n",
    "            X, y = self.shuffleData(X, y)\n",
    "            batches = [(X[i:i+batch_size], y[i:i+batch_size]) for i in range(0, int(self.training_size), batch_size)]\n",
    "            for batch in batches:\n",
    "                Xi = batch[0]\n",
    "                yi = batch[1]\n",
    "\n",
    "                self.feedForward(Xi)\n",
    "                self.backpropagation(yi)\n",
    "\n",
    "                # update weights\n",
    "                for l in range(self.num_layers-1):\n",
    "                    self.w[l] = self.w[l] - learning_rate * self.grad_w[l]\n",
    "                    self.b[l] = self.b[l] - learning_rate * self.grad_b[l]\n",
    "            \n",
    "            if output:\n",
    "                self.outputTrainingStatus(e, X, y)\n",
    "\n",
    "    def outputTrainingStatus(self, epoch_num, X, y):\n",
    "        y1 = np.nonzero(y)[1]\n",
    "        predictions = self.predict(X)\n",
    "        precision = self.evaluatePredictions(predictions, y1)\n",
    "        print(\"Epoch: {0} - precision: {1:.4f}, cost: {2:.4f}\".format(epoch_num, precision, self.cost(y)))\n",
    "\n",
    "    def shuffleData(self, X, y):\n",
    "        c = list(zip(X, y))\n",
    "        random.shuffle(c)\n",
    "        X, y = zip(*c)\n",
    "        return (np.asarray(X), np.asarray(y))\n",
    "\n",
    "    def predict(self, X):\n",
    "        self.feedForward(X)\n",
    "        return np.argmax(self.a[-1], axis=1)\n",
    "\n",
    "    def evaluatePredictions(self, predictions, y):\n",
    "        return (predictions == y).sum() / float(len(y))\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "    def sigmoidPrime(self, z):\n",
    "        return self.sigmoid(z) * (1 - self.sigmoid(z))\n",
    "    \n",
    "    \n",
    "def one_hot_encoding(output_layer_size, m, y_train):\n",
    "    y = np.zeros((m, output_layer_size))\n",
    "    for i in range(m):\n",
    "        y[i, :] = [item == y_train[i] for item in range(output_layer_size)]\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from mnist import MNIST\n",
    "\n",
    "# load the dataset\n",
    "mndata = MNIST('./data')\n",
    "images_train, labels_train = mndata.load_training()\n",
    "images_test, labels_test = mndata.load_testing()\n",
    "\n",
    "# convert to numpy arrays\n",
    "images_train = np.asarray(images_train)\n",
    "images_test = np.asarray(images_test)\n",
    "labels_test = np.asarray(labels_test)\n",
    "\n",
    "# normalize to [0,1] scale\n",
    "images_train = images_train / 255.0\n",
    "images_test = images_test / 255.0\n",
    "\n",
    "X = images_train\n",
    "y = labels_train\n",
    "\n",
    "# transform y in a vector of zeros and one. \n",
    "# I.e. number \"5\" will be = [0 0 0 0 0 1 0 0 0 0]\n",
    "output_layer_size = 10\n",
    "m = len(images_train)\n",
    "y = one_hot_encoding(output_layer_size, m, y)\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# let's create our neural network (with one hidden layer) and train it using SGD\n",
    "nn = NeuralNetwork(784, 30, 10)\n",
    "nn.stochasticGradientDescent(X, y, regularization=5.0, learning_rate=0.1, epochs=50, batch_size=10, output=True)\n",
    "\n",
    "# you can also use gradient descent\n",
    "# nn.gradientDescent(X, y, regularization=1.0, learning_rate=0.3, epochs=150, output=True)\n",
    "\n",
    "# our neural network is already trained. Now we'll check its precision on\n",
    "# identifying digits in the test set\n",
    "predictions = nn.predict(images_test)\n",
    "print(\"Precision in test: {0}\".format(nn.evaluatePredictions(predictions, labels_test)))\n",
    "\n",
    "print('Time in seconds: ' + str(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
