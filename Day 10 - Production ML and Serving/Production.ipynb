{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created by [Nimblebox Inc.](https://www.nimblebox.ai/).\n",
    "<img style=\"float:right; margin-right: 50px\" src=\"https://idroot.us/wp-content/uploads/2019/03/TensorFlow-logo.png\">\n",
    "<img style=\"float:left; margin-right: 50px\" src=\"https://media-exp1.licdn.com/dms/image/C4E1BAQH3ErUUfLXoHQ/company-background_10000/0?e=2159024400&v=beta&t=9Z2hcX4LqsxlDd2BAAW8xDc-Obfvk_rziT1AkPKBcCc\" alt=\"Nimblebox Logo\" width=\"500\" height=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Today's session attempts to shed some light into an often overlooked aspect of the Machine Learning lifecycle, that is Model Serving. This is mainly because unlike other topics discussed in this webinar series, issues and processes regarding Model serving are even newer and constantly evolving. There is no solution which fits all (or even most) use cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Idea\n",
    "The basic idea and the first step in Model serving is to freeze and export the model. A Keras model consists of multiple components:\n",
    "\n",
    "- An architecture, or configuration, which specifyies what layers the model contain, and how they're connected.\n",
    "- A set of weights values (the \"state of the model\").\n",
    "- An optimizer (defined by compiling the model).\n",
    "- A set of losses and metrics (defined by compiling the model or calling ```add_loss()``` or ```add_metric()```).\n",
    "\n",
    "The Keras API makes it possible to save of these pieces to disk at once, or to only selectively save some of them:\n",
    "\n",
    "- Saving everything into a single archive in the TensorFlow SavedModel format (or in the older Keras H5 format). This is the standard practice.\n",
    "- Saving the architecture / configuration only, typically as a JSON file.\n",
    "- Saving the weights values only. This is generally used when training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a model (which we will later export)\n",
    "For this webinar, we will be building and exporting the CNN digit classifier we built in the earlir session. Please refer to the Day 9 notebook for more information on the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in x_train 60000\n",
      "Number of images in x_test 10000\n",
      "Model: \"MNIST_CNN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 28)        280       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 28)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4732)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               605824    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 607,394\n",
      "Trainable params: 607,394\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "### Loading Data ###\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "\n",
    "### Pre-processing images ###\n",
    "# Reshaping the array to 4-dims so that it can work with the Keras API\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "# Making sure that the values are float so that we can get decimal points after division\n",
    "# x_train = x_train.astype('float32')\n",
    "# x_test = x_test.astype('float32')\n",
    "\n",
    "# Normalizing the RGB codes by dividing it to the max RGB value.\n",
    "# x_train /= 255\n",
    "# x_test /= 255\n",
    "\n",
    "# Sanity check\n",
    "print('Number of images in x_train', x_train.shape[0])\n",
    "print('Number of images in x_test', x_test.shape[0])\n",
    "\n",
    "\n",
    "### Defining the model\n",
    "# Importing the required Keras modules containing model and layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D\n",
    "# Creating a Sequential Model and adding the layers\n",
    "model = Sequential([\n",
    "                     Conv2D(28, kernel_size=(3,3), input_shape=input_shape),\n",
    "                     MaxPooling2D(pool_size=(2, 2)),\n",
    "                     Flatten(),\n",
    "                     Dense(128, activation=tf.nn.relu),\n",
    "                     Dropout(0.2),\n",
    "                     Dense(10,activation=tf.nn.softmax)\n",
    "                    ], name=\"MNIST_CNN\")\n",
    "\n",
    "## Building and compiling it\n",
    "model.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 17s 9ms/step - loss: 0.7257 - accuracy: 0.8579\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 15s 8ms/step - loss: 0.2581 - accuracy: 0.9284\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 15s 8ms/step - loss: 0.2010 - accuracy: 0.9440\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 15s 8ms/step - loss: 0.1744 - accuracy: 0.9506\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 18s 10ms/step - loss: 0.1738 - accuracy: 0.9535\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.1312 - accuracy: 0.9709\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.13116046786308289, 0.9708999991416931]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training the model\n",
    "history = model.fit(x_train, y_train, epochs=5)\n",
    "\n",
    "### And Evaluate \n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is ready, performing well and ready to ship! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/shubham/opt/anaconda3/envs/ml-env/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From /Users/shubham/opt/anaconda3/envs/ml-env/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: ./MNIST-CNN/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('./MNIST-CNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 224\n",
      "drwxr-xr-x  5 shubham  staff     160 Aug 21 07:49 \u001b[1m\u001b[36m.\u001b[m\u001b[m/\n",
      "drwxr-xr-x  6 shubham  staff     192 Aug 21 07:49 \u001b[1m\u001b[36m..\u001b[m\u001b[m/\n",
      "drwxr-xr-x  2 shubham  staff      64 Aug 21 07:49 \u001b[1m\u001b[36massets\u001b[m\u001b[m/\n",
      "-rw-r--r--  1 shubham  staff  111980 Aug 21 07:49 saved_model.pb\n",
      "drwxr-xr-x  4 shubham  staff     128 Aug 21 07:49 \u001b[1m\u001b[36mvariables\u001b[m\u001b[m/\n"
     ]
    }
   ],
   "source": [
    "%ls -la MNIST-CNN/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exporting sklearn models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install -q -y joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn import datasets\n",
    "clf = svm.SVC()\n",
    "X, y= datasets.load_iris(return_X_y=True)\n",
    "clf.fit(X, y)\n",
    "print(clf.predict(X[0:1]))\n",
    "from joblib import dump, load\n",
    "dump(clf, \"iris-svm.skl\")\n",
    "\n",
    "loaded_clf = load(\"iris-svm.skl\")\n",
    "loaded_clf.predict(X[0:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deployment \n",
    "There are many ways to deploy models, with new paradigms being introduced and updated often. Some of them have been listed here-\n",
    "1. **REST APIs** - Using web framework like [Flask](https://flask.palletsprojects.com/en/1.1.x/), [FastAPI](https://fastapi.tiangolo.com/), Django, a REST API can be built, which can make inferences on the webserver. These can be easily integrated into existing Web applications\n",
    "1. **Using TensorFlow Serving** - Based on containers, [TensorFlow Serving](https://www.tensorflow.org/tfx/guide/serving) provides a flexible, high-performance serving system for machine learning models, designed for production environments. TensorFlow Serving makes it easy to deploy new algorithms and experiments, while keeping the same server architecture and APIs.\n",
    "1. [**Kubeflow**](https://www.kubeflow.org/) Free and Open source machine learning platform designed to make deployments of machine learning (ML) workflows on Kubernetes simple, portable and scalable\n",
    "1. **On-device Inference** - Inference can be performed on the device itself using \n",
    "    1. [TensorFlow Lite](https://www.tensorflow.org/lite) allows deploying ML models on mobile (Android and iOS) and IoT devices\n",
    "    1. [TensorFlow.js](https://www.tensorflow.org/js) - Allows developing and deploying TensorFlow models using Javascript. Can be used to develop server side (using nodejs) as well as clientside apps for running inference\n",
    "    1. [Intel OpenVINO™ Toolkit](https://software.intel.com/content/www/us/en/develop/tools/openvino-toolkit.html) allows model optimisation and deployment of Computer Vision based models for variety of Intel hardware\n",
    "1. **Cloud Based Tools** - GCP AI Platform, Amazon SageMaker, Azure Machine Learning are all cloud based ML tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing Flask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flask micro web framework built with a small core and easy-to-extend philosophy. It is classified as a microframework because it does not require particular tools or libraries. It has no database abstraction layer, form validation, or any other components where pre-existing third-party libraries provide common functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a simple websrver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting flask_api_demo.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile flask_api_demo.py\n",
    "\n",
    "# load Flask \n",
    "import flask\n",
    "app = flask.Flask(__name__)\n",
    "\n",
    "\n",
    "@app.route(\"/predict\", methods=[\"GET\",\"POST\"])\n",
    "def predict():\n",
    "    data = {\"success\": False}\n",
    "    # get the request parameters\n",
    "    params = flask.request.json\n",
    "    \n",
    "    if (params == None):\n",
    "        params = flask.request.args\n",
    "        \n",
    "    # if parameters are found, echo the msg parameter \n",
    "    if (params != None):\n",
    "        data[\"response\"] = params.get(\"msg\")\n",
    "        data[\"success\"] = True\n",
    "    # return a response in json format \n",
    "    \n",
    "    return flask.jsonify(data)\n",
    "# start the flask app, allow remote connections\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(host='127.0.0.1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the development webserver\n",
    "On your local machine, run this:\n",
    "\n",
    "    FLASK_ENV=DEVELOPMEMT python3 flask_api_demo.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'response': 'hello', 'success': True}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "r = requests.post(\"http://127.0.0.1:5000/predict\", json={\"msg\": \"hello\"})\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifying the webserver to load TF Model on startup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting flask_api_tf.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile flask_api_tf.py\n",
    "\n",
    "# load Flask \n",
    "import flask\n",
    "\n",
    "# Adding TF import\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from json import loads\n",
    "\n",
    "model = None\n",
    "\n",
    "app = flask.Flask(__name__)\n",
    "\n",
    "def load_model(path=\"./MNIST-CNN\"):\n",
    "    global model\n",
    "    model = keras.models.load_model(path)\n",
    "    print(model.summary())\n",
    "    \n",
    "def make_prediction(model_in):\n",
    "    try:\n",
    "        np_image = np.asarray(loads(model_in))\n",
    "        np_image = np_image.reshape(1, 28, 28, 1)\n",
    "        pred = model.predict(np_image).argmax().item()\n",
    "    except Exception as e:\n",
    "        print(\"ERROR\")\n",
    "        print(e)\n",
    "        pred = -1\n",
    "    print(pred)\n",
    "    return pred\n",
    "\n",
    "@app.route(\"/predict\", methods=[\"POST\"])\n",
    "def predict():\n",
    "    data = {\"success\": False}\n",
    "    \n",
    "    # get the request parameters\n",
    "    params = flask.request.json\n",
    "    \n",
    "    if (params == None):\n",
    "        params = flask.request.args\n",
    "         \n",
    "    if (params != None):\n",
    "        model_input_str = params.get(\"data\")\n",
    "        resp = make_prediction(model_input_str)\n",
    "        data[\"response\"] = resp\n",
    "        data[\"success\"] = True if resp > -1 else False\n",
    "        \n",
    "    # return a response in json format \n",
    "    return flask.jsonify(data)\n",
    "\n",
    "# start the flask app, allow remote connections\n",
    "if __name__ == \"__main__\":\n",
    "    load_model()\n",
    "    app.run(host='127.0.0.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'response': 9, 'success': True}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "inp = json.dumps(x_test[12].reshape(14, 56).tolist())\n",
    "r = requests.post(\"http://127.0.0.1:5000/predict\", json={\"data\": inp})\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see what inference speeds we're getting for this simple model served via Flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.2 ms ± 726 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "inp = json.dumps(x_test[12].reshape(1, 28, 28, 1).tolist())\n",
    "r = requests.post(\"http://127.0.0.1:5000/predict\", json={\"data\": inp})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "Naturally, the next steps would be to implement better error handling, authentication and rate limiting (if required) for these Flask APIs. The in built development server we've been using to serve Flask is not suitable for production. For deploying Flask apps, please take a look at the [documentation](https://flask.palletsprojects.com/en/1.1.x/tutorial/deploy/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
